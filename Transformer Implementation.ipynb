{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiB0MWYKy1I/sMnmhTkuIA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install labml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7YdeAE69_1n","executionInfo":{"status":"ok","timestamp":1739470447665,"user_tz":300,"elapsed":11675,"user":{"displayName":"Sidney Wai","userId":"07083834017862630125"}},"outputId":"f79fe587-6d62-4559-cd80-d5f380cd398f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting labml\n","  Downloading labml-0.5.3-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from labml) (3.1.44)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from labml) (6.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from labml) (1.26.4)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->labml) (4.0.12)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.2)\n","Downloading labml-0.5.3-py3-none-any.whl (94 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: labml\n","Successfully installed labml-0.5.3\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"id":"xGxjvRCy9uj8","executionInfo":{"status":"ok","timestamp":1739470461780,"user_tz":300,"elapsed":11190,"user":{"displayName":"Sidney Wai","userId":"07083834017862630125"}}},"outputs":[],"source":["import math\n","from typing import Optional, List\n","import torch\n","from torch import nn\n","from labml import tracker"]},{"cell_type":"code","source":["class headSplit(nn.Module):\n","  def __init__(self, d_model: int, heads: int, d_k: int, bias:bool):\n","    super().__init__()\n","\n","    self.linear = nn.linear(d_model, heads*d_k, bias)\n","    self.heads = heads\n","    self.d_k = d_k\n","\n","  def forward(self, x: torch.Tensor):\n","    head_shape = x.shape[-1]\n","    x = self.linear\n","    x = x.view(*head_shape, self.heads, self.d_k)\n","    return x"],"metadata":{"id":"Zap6ml62-g9Y","executionInfo":{"status":"ok","timestamp":1739470469702,"user_tz":300,"elapsed":162,"user":{"displayName":"Sidney Wai","userId":"07083834017862630125"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MultiHeadedAttention(nn.Module):\n","  def __init__(self, heads: int, d_model: int, dropout_prob: int, bias: bool = True):\n","    super().__init__()\n","    self.heads = heads\n","    self.d_k = d_model//heads\n","\n","    self.query = headSplit(d_model, heads, self.d_k, bias = bias)\n","    self.key = headSplit(d_model, heads, self.d_k, bias = bias)\n","    self.query = headSplit(d_model, heads, self.d_k, bias = True)\n","\n","    self.softmax = nn.Softmax(dim = 1)\n","\n","    self.output = nn.linear(d_model, d_model)\n","\n","    self.dropout = nn.Dropout(p = dropout_prob)\n","\n","    self.scale = 1/ math.sqrt(self.d_k)\n","\n","    self.attention = None\n","\n","\n","\n","  def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n","    return torch.einsum('ibhd,jbhd->ijbh', query, key)\n","\n","\n","\n","  def prep_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n","    assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n","    assert mask.shape[1] == key_shape[0]\n","    assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n","\n","    mask.unsqueeze(-1)\n","\n","    return mask\n","\n","\n","\n","  def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n","      seq_len, batch_size, _ = query.shape\n","      if mask is not None:\n","        mask = self.prep_mask(mask, query.shape, key.shape)\n","\n","      query = self.query(query)\n","      key = self.key(key)\n","      value = self.value(value)\n","\n","      scores = self.get_scores(query, key)\n","\n","      scores *= self.scale\n","\n","      if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","      attention = self.softmax(scores)\n","\n","      tracker.debug('attention', attention)\n","\n","      attention = self.dropout(attention)\n","\n","      x = torch.einsum('ijbh,jbhd->ijbh', attention, value)\n","\n","      self.attention = attention.detach()\n","\n","      x = x.reshape(seq_len, batch_size, -1)\n","\n","      return self.output(x)"],"metadata":{"id":"eYtFhOI0_cmM","executionInfo":{"status":"ok","timestamp":1739470471938,"user_tz":300,"elapsed":193,"user":{"displayName":"Sidney Wai","userId":"07083834017862630125"}}},"execution_count":4,"outputs":[]}]}