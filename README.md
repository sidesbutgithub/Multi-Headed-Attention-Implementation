Implementatation of Multi-Headed Attention for a machine learning transformer as described in the paper "Attention Is All You Need"
